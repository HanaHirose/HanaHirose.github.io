<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>機械学習の勉強部屋</title>
    <link rel="stylesheet" href="styles.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <header>
        <h1>ガウス過程</h1>
        <nav>
            <ul>
                <li><a href="index.html" class="special-link">Back to Home</a></li>
            </ul>
        </nav>
        <nav>
            <ul>
                <li><a href="model_selection.html" >＜前へ：モデル、ハイパーパラメータの選択</a></li>
                <li><a href="Gaussian_process2.html" >＞次へ：ガウス過程（続き）</a></li>
            </ul>
        </nav>
    </header>
    <div>
        <h2>ガウス過程とは</h2>
         <h3>線形基底モデルの復習</h3>
        <p>
            以前、\(y(\mathbf{x},\mathbf{w})=\mathbf{w}^T \Phi(\mathbf{x})\)と仮定することで線形回帰モデルを考えた。
            観測データへのフィッティングをすることでパラメータ\(\mathbf{w}\)を決定した。
            その際、基底関数\(\Phi(\mathbf{x})\)は人間がうまく決めてやる必要があった。
        </p>
        <h3>ガウス過程の導入</h3>
        <p>
            基底関数を直接与えるのではなく、別のアプローチとしてガウシアン過程というものがある。
            これは、任意の入力データの組\(\mathbf{x}_1,\mathbf{x}_2, \dots, \mathbf{x}_N\)に対応する
            関数値\(y(\mathbf{x}_1),y(\mathbf{x}_2),\dots,y(\mathbf{x}_N)\)
            が\(N\)次元のガウス分布に従う確率変数であるとしてモデリングする方法である。
        </p>
        <p>
            平均は\(m(\mathbf{x}_1),m(\mathbf{x}_2),\dots,m(\mathbf{x}_N)\)だが、０と置く場合が多い。
        </p>
        <p>
            共分散が重要で、これは次のように書ける。
        </p>
        $$ K =
        \begin{pmatrix}
        k(\mathbf{x}_1,\mathbf{x}_1) & k(\mathbf{x}_1,\mathbf{x}_2) & \dots & k(\mathbf{x}_1,\mathbf{x}_N)\\
        k(\mathbf{x}_2,\mathbf{x}_1) & k(\mathbf{x}_2,\mathbf{x}_2) & \dots & k(\mathbf{x}_2,\mathbf{x}_N)\\
        \vdots & \vdots & \ddots & \vdots \\
        k(\mathbf{x}_N,\mathbf{x}_1) & k(\mathbf{x}_N,\mathbf{x}_2) & \dots &k(\mathbf{x}_N,\mathbf{x}_N)\\
        \end{pmatrix}
        $$
        <p>
            ただし、\(k(\mathbf{x},\mathbf{x'})\)はカーネル関数である。
            特に、ガウス過程においては共分散行列\(K\)が半正定値（すべての固有値が非負）である必要がある。
            \(k(\mathbf{x},\mathbf{x'})\)として、正定値カーネルという特別なカーネル関数を選んでやればよい。
        </p>

        <h3>正定値カーネルの例</h3>
        <p>代表的な正定値カーネルの例として、以下のような関数がある。</p>
        <h4>Gaussian カーネル</h4>
        $$ k(\mathbf{x},\mathbf{x'}) = \exp{- \gamma \|\mathbf{x}-\mathbf{x'}\|_2^2} \quad ( \gamma > 0) $$
        <h4>Matern カーネル</h4>
        $$ k(\mathbf{x},\mathbf{x'}) = \frac{1}{\Gamma(\nu)2^{\nu-1}} \left( \frac{\sqrt{2\nu}}{l} \|\mathbf{x}-\mathbf{x'} \|_2^2 \right)^\nu K_\nu \left( \frac{\sqrt{2\nu}}{l} \|\mathbf{x}-\mathbf{x'} \|_2^2 \right) $$
        <h4>Rational Quadratic カーネル</h4>
        $$ k(\mathbf{x},\mathbf{x'}) = \left( 1+ \frac{ \|\mathbf{x}-\mathbf{x'} \|_2^2 }{ 2 \alpha l^2 } \right)^{-\alpha} $$
        <h4>DotProduct カーネル</h4>
        $$ k(\mathbf{x},\mathbf{x'}) = \sigma_0^2 + \mathbf{x}\cdot \mathbf{x'} $$

        <h3>カーネル関数</h3>
        <p>ちなみに、一般にカーネル関数は基底関数\(\Phi(\mathbf{x})\)を使って次のように表せる。</p>
        $$ k(\mathbf{x},\mathbf{x'}) = \sum_{j=1}^{M} \Phi_j(\mathbf{x})\Phi_j(\mathbf{x'}) $$

    </div>    

    <div class="gallery">
        <figure>
            <img src="figures/Kernel_1D.png" alt="代表的なカーネル関数" width="800">
            <figcaption>Figure 1: 代表的なカーネル関数（\(x'\)は固定している）</figcaption>
        </figure>
    </div>

    <div class="gallery">
        <figure>
            <img src="figures/Heatmap_Kernel.png" alt="代表的なカーネル関数（ヒートマップ）" width="800">
            <figcaption>Figure 2: 代表的なカーネル関数（ヒートマップ）</figcaption>
        </figure>
    </div>

    <div>
        <h3>ガウス過程事前分布からの関数のサンプリング</h3>
        <p>\(y(\mathbf{x},\mathbf{w})=\mathbf{w}^T \Phi(\mathbf{x})\)として\(y(\mathbf{x},\mathbf{w})\)を表現するのではなく、
            次のように、カーネル関数からなる共分散行列からランダム関数\(y(\mathbf{x})\)を表現することを考える。手順は以下の通りである。
        </p>
        <p>１．適当に入力ベクトル\(\mathbf{x}_1,\mathbf{x}_2,\dots,\mathbf{x}_N\)を発生させる。</p>
        <p>２．カーネル関数を使って次のように共分散行列を作成する。\(K = (k(\mathbf{x}_n,\mathbf{x}_m))\)</p>
        <p>３．共分散行列を固有値分解する。</p>
        $$ K = U \mathrm{diag} (\lambda_i) U^T $$
        <p>４．ガウス分布\(\mathcal{N}(0,1)\)からNこの独立な乱数\(r_1,r_2,\dots,r_N\)を発生する。</p>
        <p>５．以下のようにランダム関数\(y(\mathbf{x})\)の代表点からなるベクトルを構成することができる。</p>
        $$ y(\mathbf{x}_n) = \sum_{m=1}^N U_{nm} \sqrt{\lambda_m} r_m $$
        $$ \mathbf{y}= U \mathrm{diag} (\sqrt{\lambda}_i) \mathbf{r} $$

    </div>
    
    <div class="gallery">
        <figure>
            <img src="figures/RandomFunc_Kernel.png" alt="カーネル関数から生成したランダム関数" width="800">
            <figcaption>Figure 3: カーネル関数から生成したランダム関数の様子）</figcaption>
        </figure>
    </div>
    
    <div>
        <h2>ガウス過程による回帰</h2>
        <h3>ガウス観測過程</h3>
        <p>
            これからガウス観測過程がどのようなものか見ていく。
            \(\mathbf{x}_1,\mathbf{x}_2,\dots,\mathbf{x}_N\)に対する予測値を\(y(\mathbf{x}_1),y(\mathbf{x}_2),\dots,y(\mathbf{x}_N)\)と表し、
            これをまとめてベクトルとして\(\mathbf{y}\)と書く。観測される目標変数は\(\mathbf{t}\)である。
        </p>
        <p>ざっくりいえば、\(\mathbf{y}\)はガウス過程で発生するが、さらにガウシアンノイズが乗り、最終的に\(\mathbf{t}\)になるというながれである。</p>
        <p>これを以下で詳しく見てみよう。まず、観測される目標変数には次のようにノイズ\(\epsilon\)が乗る。</p>
        $$ t_n = y_n + \epsilon_n $$
        <p>このノイズはガウス分布に従うので、次のようにあらわせる。</p>
        <h4>ガウス観測過程モデル</h4>
        $$ p(\mathbf{t}|\mathbf{y}) = \mathcal{N}(\mathbf{t}| \mathbf{y}, \beta^{-1}, \mathbf{I}_N) $$
        <p>ただし、\(\mathbf{I}_N\)は単位行列である。</p>
        <p>また、ガウス過程の定義より、周辺分布\(p(\mathbf{y})\)は次のように、平均０，共分散が\(K\)のガウス分布になる。</p>
        <h4>ガウス過程事前分布</h4>
        $$ p(\mathbf{y}) = \mathcal{N}(\mathbf{y}|0, K) $$
        <p>
            ここで、\(K\)はカーネル関数で構成されるが、これは、
            「お互いに似ている2つの点\(\mathbf{x}_n\)と\(\mathbf{x}_m\)に対して、
            対応する値\(y(\mathbf{x}_n)\)と\(y(\mathbf{x}_m)\)が高い相関を持つ」
            ということを表すように決める。
        </p>
        <h4>出力\(\mathbf{t}\)の周辺分布</h4>
        <p>上で見た二つのガウス分布を合わせて積分することで出力\(\mathbf{t}\)の周辺分布が次のように分かる。</p>
        $$ p(\mathbf{t}) = \int p(\mathbf{t}|\mathbf{y}) p(\mathbf{y}) d\mathbf{y} = \mathcal{N} (\mathbf{t}|0,C_N) $$
        <p>このように、ガウシアンとガウシアンを掛け合わせているので、\(p(\mathbf{t}) \)もガウシアンになっている。
            ただし、共分散\(C_N\)は、次で与えられ、単純に二つのガウス分布の共分散の足し合わせになっている。</p>
        $$ C_N = \beta^{-1} \mathbf{I}_N + K $$
        $$ C_N(\mathbf{x}_n,\mathbf{x}_m) = \beta^{-1} \delta_{nm} + k(\mathbf{x}_n,\mathbf{x}_m) $$

        <h3>ガウス過程回帰を用いた新しい入力に対する予想</h3>
        <p>
            やりたいことは次の通り。訓練集合として入力 \(\mathbf{x}_1,\mathbf{x}_2,\dots,\mathbf{x}_N\)と
            それに対応する出力値\(\mathbf{t}_N = (t_1,t_2, \dots, t_n) \)が与えらえているときに、
            新しい入力\(\mathbf{x}_{N+1}\)に対する目標変数\(t_{N+1}\)を予想したい。
        </p>
        $$ \mathbf{t}_{N+1} = (\mathbf{t}_{N}^T, t_{N+1})^T = (t_1, t_2, \dots, t_N, t_{N+1}) $$

        <p>
            そのためには、予測分布、つまり\(t_{N+1}\)の事後分布が分かればいい。
            これは多変量正規分布の公式を使って計算すると次のように得られる。
        </p>
        $$ p(t_{N+1}|\mathf{t}_N) = \frac{p(\mathbf{t}_{N+1})}{p(\mathbf{t}_{N})} = \mathcal{N} (t_{N+1}| m_{N+1}, \sigma_{N+1}^2) $$
        <p>ただし、</p>
        $$ m_{N+1} = \mathbf{k}^T C_N \mathbf{t}_N $$
        $$ \sigma_{N+1}^2 = \beta^{-1} + k(\mathbf{x}_{N+1},\mathbf{x}_{N+1}) - \mathbf{k}^T C_N^{-1} \mathbf{k} $$
        <p>である。これを導出する途中で、結合分布</p>
        $$ p(\mathbf{t}_{N+1} ) = \mathcal{N}(\mathbf{t}_{N+1}|0, C_{N+1}) $$
        $$ C_{N+1} = 
        \begin{pmatrix}
        C_N & \mathbf{k} \\
        \mathbf{k}^T & c \\
        \end{pmatrix}
        $$
        <p>を使った。ただし、\(c=\beta^{-1}+k(\mathbf{x}_{N+1},\mathbf{x}_{N+1}) \)である。</p>
        <p>以上のように、無事、誤差付きで\(\mathbf{t}_N\)から\(\mathbf{t}_{N+1}\)を回帰することができた。</p>

        <h3>ガウス過程回帰におけるハイパーパラメータの選択</h3>
        <p>
            多くの場合、ガウス過程回帰ではカーネル関数や観測過程モデルにハイパーパラメータが含まれている。
            それらを決める方法として代表的なのは周辺尤度最大化である。
        </p>
        <h4>周辺尤度最大化</h4>
        <p>ハイパーパラメータを\(\theta\)と書いて、尤度関数\(p(\mathbf{t}|\theta)の対数を取ったものを評価し、それを最大化するようにハイパーパラメータを\(\theta\)を決める。</p>
        <p>対数尤度関数はつぎのように書ける。</p>
        $$ \ln p(\mathbf{t}_N) = \frac{1}{2} \mathbf{t}_N^T C_N^{-1}\mathbf{t}_N - \frac{1}{2} \ln \det C_N - \frac{N}{2} \ln (2 \pi) $$
        <p>ここで\(C_N\)がハイパーパラメータに依存する。</p>

        <h3>ガウス過程による回帰の例</h3>
        <P>下図は正弦関数をもとに発生させたデータに対してガウス過程を適応した結果である。
            青の丸い点がデータ点を表し、緑の線がガウス過程で得られた予測分布の平均で、影のついた領域は平均から標準偏差の2倍までの領域である。
            影の領域の縦幅はどれくらいその予測に自信があるかを表していて、グラフ右の方ではデータ点があまりないので
            影のついた領域が広くなっており、その辺りは自信があまりないということが分かる。</P>

    </div>

    <div class="gallery">
        <figure>
            <img src="figures/Gaussian_Process_Regression.png" alt="ガウス過程による回帰" width="800">
            <figcaption>Figure 4: ガウス過程による回帰</figcaption>
        </figure>
    </div>

    <h3>線形モデルとガウス過程の比較</h3>
    <p>単純に線形モデルで最小二乗法を用いた場合は表現できる関数形が限られており、パラメータの変化が関数全体に影響する。</p>
    <p>
        一方、ガウス過程と周辺尤度を最大化する場合、多様な関数形を表現できる。また、関数形を局所的に変化させることができるし、データの粗密性に依存して信頼性も評価できる。
        つまり、データがたくさんある辺りでは自信まんまんで、データが疎のところでは自信がないということを表現してくれる。
    </p>

    <div>
        <h2>実装の例</h2>
        <p>以下では代表的なカーネル関数の様子やガウス過程による回帰の例を示している。</p>
    </div>

    <div class="github-link">
        <a href="https://github.com/HanaHirose/ML_Self_Study/tree/main/Gaussian_Process" target="_blank">
            <div class="link-header">
                <img src="figures/github-mark.png" alt="GitHub Logo">
                <span>GitHub：カーネル関数とガウス過程による回帰と分類</span><br>    
            </div>
            <p class="link-url">https://github.com/HanaHirose/ML_Self_Study/tree/main/Gaussian_Process</p>
        </a>
    </div>
    
    <footer>
        <nav>
            <ul>
                <li><a href="model_selection.html" >＜前へ：モデル、ハイパーパラメータの選択</a></li>
                <li><a href="Gaussian_process2.html" >＞次へ：ガウス過程（続き）</a></li>
            </ul>
        </nav>
        <nav>
            <ul>
                <li><a href="index.html" class="special-link">Back to Home</a></li>
            </ul>
        </nav>

        <p>© 2024 Hana Hirose. All rights reserved.</p>
    </footer>
</body>
</html>