<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>機械学習の勉強部屋</title>
    <link rel="stylesheet" href="styles.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <header>
        <h1>サポートベクトルマシン（ソフトマージン）</h1>
        <nav>
            <ul>
                <li><a href="index.html" class="special-link">Back to Home</a></li>
            </ul>
        </nav>
        <nav>
            <ul>
                <li><a href="SupportVectorMachine.html" >＜前へ：サポートベクトルマシン</a></li>
                
            </ul>
        </nav>
    </header>
    <div>
        <h2>ソフトマージン</h2>
        <h3>過学習を防ぐには</h3>
        <p>
            前ページでは訓練データが特徴空間において線形分離可能であることを仮定してきた。しかし、現実には完璧に分離できずクラスの条件付き確率分布が重なる場合もある。
            そのような場合に無理して完全な線形分離を求めると、過学習が生じて汎化性能を悪化させてしまう。
            そこで、誤りを適当に許容させて学習させることが重要である。これはパーセプトロンの時代にはできなかったことである。
        </p>
        <h3>ソフトマージン</h3>
        <p>
            過学習を防ぐために、制約を満たさない条件は許さない（ハードマージン）のではなく、許すがその代わり罰金を加える（ソフトマージン）という手法がある。
        </p>
        <p>
            ハードマージンでは誤って分類したデータについては無限大のペナルティを与え、正しく分類したデータにはペナルティを与えない誤差関数を暗に用いて、マージンを最大化するようにパラメータを最適化している。
            これを変更して、データがマージン内に侵入した時は、マージン境界からの距離に応じてペナルティを与えることで、マージン境界の「間違った側」に存在することを許すように修正することを考える。
        </p>
        <p>
            具体的には、まずスラック変数\(\xi_n \geqslant 0 (n=1, \dots N)\)を導入する。
            スラック変数は各訓練データごとに定義される変数で、データが正しく分類されてかつマージンの境界上か外側に存在するときは\(\xi_n=0\)、
            それ以外の場合には\(\xi_n=|t_n-y_n(\mathbf{x}_n)|\)として定義される。したがって、ちょうど分類境界\(y(\mathbf{x})=0\)にあるデータについては\(\xi_n=1\)、
            誤分類されたデータについては\(\xi_n>1\)が成り立つ。
        </p>
    </div>

    </div>
    <div class="gallery">
        <figure>
            <img src="figures/Figure7.3.jpg" alt="ソフトマージン" width="400">
            <figcaption>Figure 1: ソフトマージン（C.M. Bishop, PRML, 2006）</figcaption>
        </figure>
    </div>

    <div>
        <p>
            上図のように、\(\xi_n=0\)となるデータは正しく分類されており、かつマージンの境界の上か正しい側にある。また、\(0<\xi_n \leqslant 1\)となるデータはマージンの内側にあるが正しく分類されている。
            そして、\(\xi_n>1\)となるデータは分類境界に足して誤った側にあり、誤分類されている。
        </p>
        <h3>解くべき問題</h3>
        <p>
            ペナルティを課すことを反映して、制約条件のもとに最小化する目的関数は次のように変更される。
        </p>
        $$ \underset{\mathbf{w},b,\mathbf{\xi}} {\operatorname{min}} \frac{1}{2} \| \mathbf{w} \|^2 + C \sum_{n=1}^N \xi_n
        \quad \operatorname{subj.to} \xi_n \geqslant 1- t_n ( \mathbf{w}^T \Phi (\mathbf{x}_n)+b) ,\quad \xi_n \geqslant 0 \quad (n=1, \dots , N) $$
        <p>
            ここで\(C>0\)はペナルティとマージンの大きさの間のトレードオフを制御するパラメータである。
            \(C\)は訓練エラーとモデルの複雑さのトレードオフを制御する正則化係数（の逆数）のような役割を果たしている。
            実際、\(C\to \infty\)の極限においては、分離可能なハードマージンSVMの最適化問題と等しくなる。
        </p>

        <h3>双対問題への書き換え</h3>
        <p>
            ハードマージンの時と同様にラグランジュ関数を作り、\(\mathbf{w},b,\mathbf{\xi}\)に関して１回微分を０とおいて停留条件をもとめて、ラグランジュ関数に代入してそれらの変数を消去すると、以下のような双対問題に書き換えられる。
        </p>
        $$ \underset{\mathbf{a}} {\operatorname{max}} \left\{ \sum_{n=1}^N a_n - \frac{1}{2} \sum_{n=1}^N a_n \sum_{m=1}^N a_n a_m t_n t_m k(\mathbf{x}_n,\mathbf{x}_m) \right\} $$
        $$ \operatorname{subj.to} C \geqslant a_n \geqslant 0 \quad (n=1, \dots , N), \quad  \sum_{n=1}^N t_n a_n =0 $$
        <p>
            上式をみてわかる通り、結局、ハードマージンとの違いは、\(a_n\)に上限の制約\(C\geqslant a_n\)がついただけである。
        </p>
        <h3>KKT条件の解釈</h3>
        <p>
            ラグランジュ関数から得られたKKT条件から、最終的に次のようなことが分かる。
        </p>
        <p>
            \(a_n=0\)となるデータ点は識別関数\(y(\mathbf{x})\)に何も影響を及ぼさない。それ以外の点がサポートベクトルになる。
            つまり、\(a_n>0\)となる点がサポートベクトルになるということである。
        </p>
        <p>
            \(0< a_n < C\)となるデータ点は\(\xi_n=0\)が成立するので、ちょうどマージン境界上に存在している。
        </p>
        <p>
            \(a_n=C\)となるデータ点はマージン内に侵入しており、\(\xi_n \leqslant 1\)の場合は正しく分類されているが、\(\xi_n > 1\)の場合は誤分類されている。
        </p>

        <p>下図は分類不可能な２次元データに対してソフトマージンSVMを適用した例である。丸で囲まれた点がサポートベクトルである。</p>



    </div>

    <div class="gallery">
        <figure>
            <img src="figures/Figure7.4.jpg" alt="ソフトマージンSVM" width="400">
            <figcaption>Figure 2: ソフトマージンSVM（C.M. Bishop, PRML, 2006）</figcaption>
        </figure>
    </div>

    <div>
        <h2>ハイパーパラメータ選択</h2>
        <p>
            過学習を防ぐには、SVMに含まれるハイパーパラメータを適切に決定することが重要である。ハイパーパラメータにはカーネルのパラメータやソフトマージン罰金係数\(C\)などがある。
            ハイパーパラメータの選択には色々な方法があるが、交差検証法を用いるのが一般的である。つまり、いくつか\(C\)を用意しておいて、各\(C\)に対してcross-validationを計算して、一番よさそうなものを選ぶという方法である。
            ただし、交差検証誤差は解析的に表現された関数ではないので、グリッドサーチ（用意した候補点に対する全探索）で最適値を得なければいけない。
            これは、ガウス過程ではニュートン法や勾配法により対数周辺尤度の最大化ができたのとは対照的である。
        </p>

        <h2>多クラス分類への拡張</h2>
        <p>
            他クラスへ拡張する方法として、いくつかあるが、例えばOne-versus-the-restがある。これは対応するクラスはラベル１、その他のクラスはラベル０とし、分類する手法である。
            しかし、誤分類により、１が２個以上出力されたり、あるいは１つも出力されない場合がでてくるので、この問題を避けるために、新しい入力に対して
        </p>
        $$ y(\mathbf{x}) = \underset{k} {\operatorname{max}} y_k(\mathbf{x}) $$
        <p>に従って、もっとも大きな識別関数の値を返すクラスを予測値とするという工夫がある。</p>
        <p>
            他には、one-versus-oneという方式もある。
            これは、すべてのクラスの組み合わせについて２クラスSVMを学習し、その結果得られた\(K(K-1)/2\)個の分類器を適応して、「投票」が一番多かったものを最終的な出力にするというものである。
            しかし、\(K(K-1)/2\)個の分類器が必要なので、計算コストが高いという問題点はある。
        </p>
    </div>




    <footer>
        <nav>
            <ul>
                <li><a href="SupportVectorMachine.html" >＜前へ：サポートベクトルマシン</a></li>
            </ul>
        </nav>
        <nav>
            <ul>
                <li><a href="index.html" class="special-link">Back to Home</a></li>
            </ul>
        </nav>

        <p>© 2024 Hana Hirose. All rights reserved.</p>
    </footer>
</body>
</html>