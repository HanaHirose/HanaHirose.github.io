<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Topic 1</title>
    <link rel="stylesheet" href="styles.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <header>
        <h1>線形回帰</h1>
        <nav>
            <ul>
                <li><a href="index.html">Back to Home</a></li>
                <!-- <li><a href="category1.html">Back to Category 1</a></li> -->
            </ul>
        </nav>
    </header>
    <div>
        <h2>線形回帰とは</h2>
        <p>\(\mathbf{x} \in \mathbb{R}^D, \mathbf{t} \in \mathbb{R} \)</p>
        <p>ここで\(\mathbf{x}\)は説明変数（入力）、\(\mathbf{t}\)は応答変数、目的変数（出力）</p>
        <h3>回帰問題：やりたいこと</h3>
        <p>\(\mathbf{N}\)個の観測データ\( \{(\mathbf{x}_1,\mathbf{t}_1),(\mathbf{x}_2,\mathbf{t}_2), \dots ,(\mathbf{x}_N,\mathbf{t}_N) \} \)から\(\mathbf{x} \)と\(\mathbf{t}\)の関係をモデル化したい</p>
        <h3>線形回帰（Linear Regression）</h3>
        <p>最も単純な線形回帰モデルは入力変数の線形結合で\(\mathbf{t}\)を予測できるような関数\(y(\mathbf{x})\)を次のように構成することである。</p>
        $$ y(\mathbf{x},\mathbf{w})=w_0 + w_1 x_1 + w_2 x_2 + \dots + w_D x_D $$
        <p>ここで重要なのは、これはパラメータ\( \mathbf{w} \)に関して線形結合になっているという点である。だから"線形"回帰と呼ぶ。</p>
        <h3>線形基底関数モデルへの拡張</h3>
        <p>先ほどのままでは\(\mathbf{x} \)についても線形結合になっているため、表現力に乏しいモデルになっている。<br />
            そこで、入力変数に関して非線形な関数の線形結合を考えることで、このモデルを次のように拡張することができる。
        </p>
        $$ y (\mathbf{x},\mathbf{w}) = \sum_{j=0}^{M-1} w_j \phi_j(\mathbf{x}) = \mathbf{w}^T \phi(\mathbf{x}) $$
        <p>ここで、パラメータ\( \mathbf{w} = ( w_0, \dots, w_{M-1} )^T\)、基底関数 \(\phi = (\phi_0, \dots, \phi_{M-1}) \)である。<br />
        （\(\mathbf{x}\)自体はD次元だが、\(\mathbf{x}\)で構成された既定関数\(\phi\)は異なるM次元をもって良いことに注意）
        </p>
        <p>基底関数として\( \mathbf{x} \)の多項式を取るものの他に、
            よく使われる基底関数としては、例えば、以下のようなガウス基底関数がある。</p>
        $$ \phi_j(x) = exp\{-\frac{(x-\mu_j)^2)}{2s^2} \} $$
        <p>他には、次のようなシグモイド基底関数もある。</p>
        $$ \phi_j(x) = \frac{1}{1+ exp(\frac{x-\mu_j}{s})} $$
        <p>ただし、次の関数はロジスティックシグモイド関数（logistic sigmoid function）とよばれる。</p>
        $$ \sigma (a) = \frac{1}{1+ exp(-a)} $$
        <p>以下に各基底関数の様子を示す。（図：C.M. Bishop, PRML, 2006）</p>
    </div>
    <div class="gallery">
        <figure>
            <img src="figures/Figure3.1a.jpg" alt="多項式" width="300">
            <figcaption>Figure 1-1: 多項式</figcaption>
        </figure>
        <figure>
            <img src="figures/Figure3.1b.jpg" alt="ガウス基底関数" width="300">
            <figcaption>Figure 1-2: ガウス基底関数</figcaption>
        </figure>
        <figure>
            <img src="figures/Figure3.1c.jpg" alt="シグモイド関数" width="300">
            <figcaption>Figure 1-3: シグモイド関数</figcaption>
        </figure>
    </div>
    <div>
        <h3>線形回帰の最尤推定</h3>
        <p>予測したい変数である目標変数\(y\)は次のように表せる。</p>
        $$ t = y(\mathbf{x},\mathbf{w}) + \epsilon $$
        <p>ただし、\(\epsilon\)は期待値が０で精度（分散の逆数）が\(\beta\)のガウス確率変数である。</p>
        <p>すなわち、条件付き確率密度はガウス分布（または正規分布、Gaussian / normal distribution）で以下のように表せる。</p>
        $$ p(t|\mathbf{x},\mathbf{w},\beta) = \mathcal{N}(t|y(\mathbf{x},\mathbf{w}, \beta^{-1}) $$
        <h4>確率分布とは</h4>
        <p>ここで、基本事項について確認しておくと、確率分布\(p(x)\)は連続変数\(x\)において確率を考えるための概念であり、</p>
        <p>実数値\(x\)が区間\( (x, x+ \delta x) \)に入る確率が\( p(x) \delta x\)で与えられ、</p>
        $$ \int_{-\infty}^{\infty} p(x) dx = 1$$
        <p>を満たす。</p>
        <h4>ガウス分布とは</h4>
        <p>またガウス分布は以下で定義される。</p>
        $$ \mathcal{N}(x|\mu, \sigma^2) = \frac{1}{(2\pi \sigma^2)^{1/2}} exp \{-\frac{1}{2\sigma^2}(x- \mu)^2\} $$
        <p>ただし、\(\mu\)は平均（mean）、\(\sigma^2\)は分散（variance）をあらわす。分散の逆数\(\beta = 1/ \sigma^2\)は精度パラメータと呼ばれる。</p>
    </div>
    <div class="gallery">
        <figure>
            <img src="figures/Figure1.13.jpg" alt="ガウス分布" width="300">
            <figcaption>Figure 2: ガウス分布（C.M. Bishop, PRML, 2006）</figcaption>
        </figure>
    </div>
    <div>
        <h4>最尤推定について</h4>
        <p>本題に戻って、\(y(\mathbf{x},\mathbf{w})\)は以下のように表せる。</p>
        $$ y(\mathbf{x},\mathbf{w}) = \mathbb{E}[t|\mathbf{x}] = \int t p(t|\mathbf{x}) dt $$
        <p>ここで与えられたデータ\( \{ (\mathbf{x}_1,t_1),(\mathbf{x}_2,t_2), \dots, (\mathbf{x}_N,t_N) \} \)
            に対するもっともらしさとして次のような尤度関数（ゆうどかんすう）を考える。
        </p>
        $$ p(\mathbf{t}|\mathbf{X},\mathbf{w},\beta) = \prod_{n=1}^{N} \mathcal{N}(t_n | \mathbf{w}^T \phi(\mathbf{x}_n),\beta^{-1}) $$
        <p>与えられたデータ\(\mathbf{X},\mathbf{t}\)に対してこの尤度関数を最大化するようにパラメータ\(\mathbf{w}, \beta^{-1}\)を求める。</p>
        <h4>ガウスモデルの最尤法は最小二乗法</h4>
        <p>尤度関数の対数をとると次のように変形できる。</p>
        $$ 
        \begin{align*}
        In p(\mathbf{t}|\mathbf{X},\mathbf{w},\beta) 
        &= \sum_{n=1}^{N} In \mathcal{N}(t_n | \mathbf{w}^T \phi(\mathbf{x}_n),\beta^{-1}) \\
        &= \frac{N}{2} In \beta - \frac{N}{2} In (2 \pi) - \beta E_D (\mathbf{w})
        \end{align*}
        $$
        <p>
            ただし、二乗和誤差関数は
        </p>
        $$ E_D (\mathbf{w}) = \frac{1}{2} \sum_{n=1}^{N} \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \}^2 $$
        <p>によって定義される。</p>
        <p>こう書き下すことで、これを最大化するパラメータ\(\mathbf{w}, \beta^{-1}\)を求められる。</p>
        <h4>\(\mathbf{w}\)について</h4>
        <p>まずはパラメータ\(\mathbf{w}\)について最大化する。
            \(E_D (\mathbf{w}) \)を最大化すればいいので、\(\mathbf{w}\)について微分して勾配を求める。</p>
        $$ \frac{\partial}{\partial \mathbf{w}} In p(\mathbf{t}|\mathbf{X},\mathbf{w},\beta)
         = \beta \sum_{n=1}^{N} \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \} \phi(\mathbf{x}_n)^T $$
         <p>この勾配が０になるときを考えればいいので、上記式を０とおき解くと</p>
         $$ \mathbf{w}_{ML} = \mathbf{\Phi}^{\dag} \mathbf{t} $$
         <p>を得る。これは最小二乗問題の正規方程式（normal equation）として知られる。
            ここで、\( N \times M \)行列\(\mathbf{\Phi} \)は計画行列（design matrix）と呼ばれ、要素は\( \Phi_{nj} = \phi_j(\mathbf{x}_n)で与えられる。</p>
        <p>
            また、\(\mathbf{\Phi}^{\dag} \)は
        </p>
        $$ \mathbf{\Phi}^{\dag} =(\mathbf{\Phi}^T \mathbf{\Phi})^{-1} \mathbf{\Phi}^T $$
        <p>で定義され、ムーア・ペンローズの疑似行列（Moore-Penrose pseudo-inverse matrix）と呼ばれる。
            これは通常の逆行列の概念を非正方行列に拡張したものとみなせる。
            実際、\( \mathbf{\Phi}\)が正方行列の時、\(\mathbf{\Phi}^{\dag} = \mathbf{\Phi}^{-1}\)が成り立つ。
        </p>
    </div>



    <footer>
        <p>© 2024 Hana Hirose. All rights reserved.</p>
    </footer>
</body>
</html>
