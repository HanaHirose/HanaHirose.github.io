<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>機械学習の勉強部屋</title>
    <link rel="stylesheet" href="styles.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <header>
        <h1>サポートベクトルマシン</h1>
        <nav>
            <ul>
                <li><a href="index.html" class="special-link">Back to Home</a></li>
            </ul>
        </nav>
        <nav>
            <ul>
                <li><a href="Perceptron.html" >＜前へ：線形分離とパーセプトロン</a></li>
                <li><a href="Gaussian_process2.html" >＞次へ：ガウス過程（続き）</a></li>
            </ul>
        </nav>
    </header>

    <div>
        <h2>VC次元</h2>
        <p>
        入力ベクトルの次元が\(D\)のパーセプトロンはN個のベクトルの組に対して、
        \(N \leq D+1\)ならば、\(2^N\)通りの任意のラベル付けを表現できる。
        </p>
        <p>
            例えば、２次元では３点までは任意のラベル付けが可能だが、４点以上だと線形分離で表現できないラベル付けが存在する。
        </p>
        <p>
            任意のラベル付けが可能なベクトル数の上限\(D+1\)を\(D\)次元パーセプトロンに関するVC次元（Vapnik-Chervonenkis）という。
            VC次元は分類器の能力を特徴づける重要な指標である。
        </p>

        <h2>特徴空間への写像</h2>
        <p>
            上でみたVC次元の性質を利用するために、十分高次元の「特徴空間」に写像して、そこでパーセプトロンを使って線形分離すれば、
            任意の２クラス分類が学習できるだろう。この後に見ていくのは、このような考えに基づいたサポートベクトルマシン（SVM: supprot vector machine）という手法である。
        </p>

        <h2>サポートベクトルマシン（SVM）</h2>
        <p>まず、入力\(\mathbf{x}\)を次のように\(\Phi(\mathbf{x})\)を使って高次元にマップし、それをもとに出力\(y\)を得る。
        </p>
        $$ y(\mathbf{x}) = \mathbf{w}^T \Phi(\mathbf{x}) +b $$
        <p>ここで\(\Phi(\mathbf{x})\)は固定された特徴空間変換関数であり、\(b\)はバイアスパラメータである。</p>
        <p>訓練データは\(\mathbf{x}_1, \dots \mathbf{x}_N\)のＮ個の入力とそれに対応する目標値\(t_1, \dots, t_N (t_n \in \{-1,1\})\)からなり、
            未知のデータ点\(\mathbf{x}\)は\(y(\mathbf{x})\)の符合に応じて分類される。
        </p>
        <h3>マージン最大化</h3>
        <p>高次元にマップした後は、正しい分類をする超平面はたくさんある。
            そこで、真ん中ぐらいを通るところで境界線を引くのがよさそうということで、次のように「マージン最大化」を行う。
            マージンは分類境界と最も近くのデータ点までの距離として定義される。
        </p>
    </div>

    <div class="gallery">
        <figure>
            <img src="figures/Figure7.1a.jpg" alt="マージン" width="400">
            <figcaption>Figure 1: マージン（C.M. Bishop, PRML, 2006）</figcaption>
        </figure>
    </div>

    <div>
    
        <p>
            当面の間、訓練データは特徴空間で線形分離可能だという仮定のもとで話を進める。
            つまり、少なくとも一組のパラメータ\(\mathbf{w}とb\)が存在して、
        </p>
        <p>\(t_n = +1\)である点については\(y(\mathbf{x}_n)>0\)</p>
        <p>であり</p>
        <p>\(t_n = -1\)である点については\(y(\mathbf{x}_n)< 0\)</p>
        <p>が成立すると仮定する。これは、</p> 
        $$ t_n y(\mathbf{x}_n) > 0 $$
        <p>とまとめて表すことが出来る。</p> 
        <p>超平面\(y(\mathbf{x}) = 0\)から点\(\mathbf{x}\)までの距離は一般に\(|y(\mathbf{x})| / \|\mathbf{w}\|\)で与えらえる。</p>
        <p>今はすべてのデータ点を正しく分類することを考えているので、すべての\(n\)について\(t_n y(\mathbf{x}_n)\)が成り立つ。</p>
        <p>したがって、分類境界から点\(\mathbf{x}_n\)までの距離は次のようにあらわせる。</p>
        $$ \frac{t_n y(\mathbf{x}_n)}{\|\mathbf{w}\|} = \frac{t_n ( \mathbf{w}^T \Phi (\mathbf{x}_n)+b)}{\|\mathbf{w}\|}$$
        <p>マージンは訓練データと分離境界の最短距離であり、求めたいのは、そのマージンを最大化するパラメータ\(\mathbf{w}とb\)である。</p>



    </div>




    
    <footer>
        <nav>
            <ul>
                <li><a href="Perceptron.html" >＜前へ：線形分離とパーセプトロン</a></li>
            </ul>
        </nav>
        <nav>
            <ul>
                <li><a href="index.html" class="special-link">Back to Home</a></li>
            </ul>
        </nav>

        <p>© 2024 Hana Hirose. All rights reserved.</p>
    </footer>
</body>
</html>